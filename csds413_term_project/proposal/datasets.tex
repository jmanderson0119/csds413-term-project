\section*{Dataset(s)}

The \href{https://www.kaggle.com/datasets/mtesconi/twitter-deep-fake-text}{TweepFake dataset} is a collection of approximately 25,000 tweets split 50/50 between human-written posts and AI-generated posts. The AI-generated posts were produced and published to Twitter using 23 bot accounts as part of a research initiative to aid in the exploration of deepfake detection systems for social media: \href{https://doi.org/10.1371/journal.pone.0251415}{https://doi.org/10.1371/journal.pone.0251415}\\

Below are the five features we intend to extract from each tweet for the purpose of our analysis. The dataset prior to extracting these features is cleaned of any URLs, mentions, and hashtags, as from initial inspection, bot tweets don't appear to include these and inclusion may also throw off extraction:

\subsection*{Vocabulary Richness ($ V $)}
$ V $ measures the ratio of unique words in tweet $ i $ to the total number of words in tweet $ i $: 

$$ V_i = \frac{\text{total unique words in tweet i}}{\text{total words in tweet i}} $$ 

\subsection*{Sentence Length ($ S $)}
$ S $ is the average length of each sentence in a tweet, delimited by standard punctuation symbols as well as newline characters:

$$ S_i = \frac{\text{total words in tweet i}}{\text{total sentences in tweet i}} $$

\subsection*{Word Length ($W$)}
$ W $ is the average length of each word in a tweet:

$$ W_i = \frac{1}{n_{\text{words}}} \sum_{j=1}^{n_{\text{words}}} \text{len}(\text{word}_j) $$

\subsection*{Function Word Frequency ($F$)}

$ F $ captures the ratio of function words in a tweet to total words in a tweet:

$$ F_i = \frac{\text{total function words in tweet i}}{\text{total words in tweet i}} $$

\subsection*{Capitalization Abnormality ($C$)}
$ C $ measures the ratio of words containing abnormal capitalization patterns:

$$ C_i = \frac{\text{total words with non-standard capitalization in tweet i}}{\text{total words in tweet i}} $$
